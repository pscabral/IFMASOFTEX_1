{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNe2VlKtCLUYo/MQ39OsW0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pscabral/IFMASOFTEX_1/blob/main/ModeloYOLO8_2.00.ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mega.py\n",
        "\n",
        "from mega import Mega\n",
        "\n",
        "# Crie uma inst√¢ncia da classe Mega\n",
        "mega = Mega()\n",
        "\n",
        "# Cole o link completo do Mega.nz (incluindo a chave)\n",
        "url = \"https://mega.nz/file/tu0GXQzL#BGoVzch2_6s2EEQYLfkwpct1Uq-ZrRL2sHCOY54vohs\"\n",
        "\n",
        "# Use o link completo para fazer o download\n",
        "mega.download_url(url)\n",
        "\n",
        "!unzip vegetables.v4i.yolov8.zip"
      ],
      "metadata": {
        "id": "36uj1icysnwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXQuRMpvq_g2"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import pathlib\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "from torchvision import transforms, utils\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO, utils"
      ],
      "metadata": {
        "id": "Jia_tt9PrEej"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"yolov8n.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KFukV44rG7A",
        "outputId": "204acb32-c0db-46c1-c677-dc38c64f8ab3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.23M/6.23M [00:00<00:00, 132MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    CHECKPOINT_DIR = 'checkpoints'\n",
        "    EXPERIMENT_NAME = '/content/vegetables'\n",
        "    DATA_DIR = '../content/vegetables'\n",
        "    TRAIN_IMAGES_DIR = 'train/images'\n",
        "    TRAIN_LABELS_DIR = 'train/labels'\n",
        "    VAL_IMAGES_DIR = 'valid/images'\n",
        "    VAL_LABELS_DIR = 'valid/labels'\n",
        "    TEST_IMAGES_DIR = 'test/images'\n",
        "    TEST_LABELS_DIR = 'test/labels'\n",
        "    CLASSES = ['apple', 'banana', 'bell_pepper', 'carrot', 'cauliflower', 'chillies', 'cucumber', 'garlic', 'grapes', 'mango', 'mushroom', 'onion', 'orange', 'pear', 'potato', 'promegranate', 'tomato', 'watermelon']\n",
        "    NUM_CLASSES = len(CLASSES)\n",
        "    DATALOADER_PARAMS = {'batch_size': 16, 'num_workers': 2}"
      ],
      "metadata": {
        "id": "dQwe8ZDos7_y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()"
      ],
      "metadata": {
        "id": "93X8Ts0ptDs3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_dir = os.path.join(config.DATA_DIR, config.TRAIN_IMAGES_DIR)\n",
        "train_labels_dir = os.path.join(config.DATA_DIR, config.TRAIN_LABELS_DIR)\n",
        "\n",
        "num_images = 5\n",
        "\n",
        "image_files = [f for f in pathlib.Path(train_images_dir).iterdir() if f.is_file()]\n",
        "\n",
        "random.shuffle(image_files)\n",
        "\n",
        "selected_image_files = image_files[:num_images]\n",
        "\n",
        "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255), (255, 0, 255),\n",
        "          (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0), (0, 128, 128), (128, 0, 128),\n",
        "          (64, 0, 0), (0, 64, 0), (0, 0, 64), (64, 64, 0), (0, 64, 64), (64, 0, 64)]\n",
        "\n",
        "for selected_image_file in selected_image_files:\n",
        "    demo_image = selected_image_file\n",
        "\n",
        "    demo_label = pathlib.Path(train_labels_dir) / f\"{selected_image_file.stem}.txt\"\n",
        "\n",
        "    image = cv2.imread(str(demo_image))\n",
        "\n",
        "    class_list = config.CLASSES\n",
        "\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    with open(demo_label, \"r\") as file1:\n",
        "        for line in file1.readlines():\n",
        "\n",
        "            split = line.split(\" \")\n",
        "\n",
        "            class_id = int(split[0])\n",
        "\n",
        "            if 0 <= class_id < len(colors):\n",
        "\n",
        "                color = colors[class_id]\n",
        "                clazz = class_list[class_id]\n",
        "\n",
        "                x, y, w, h = map(float, split[1:5])\n",
        "\n",
        "                box = [int(x * width), int(y * height), int(w * width), int(h * height)]\n",
        "\n",
        "                cv2.rectangle(image, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), color, 2)\n",
        "\n",
        "                cv2.rectangle(image, (box[0], box[1] - 20), (box[0] + box[2], box[1]), color, -1)\n",
        "\n",
        "                cv2.putText(image, clazz, (box[0], box[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
        "\n",
        "            else:\n",
        "                print(f\"ID da classe inv√°lido: {class_id}. Certifique-se de que est√° dentro da faixa [0, {len(colors) - 1}].\")\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    image = cv2.resize(image, (600, 600))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sd-8otFVtEL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/vegetables/data.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7Lo2iYKuAkB",
        "outputId": "46ca66df-e888-45fd-fcb7-4320fdb58a8c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: ../train/images\n",
            "val: ../valid/images\n",
            "test: ../test/images\n",
            "\n",
            "nc: 18\n",
            "names: ['apple', 'banana', 'bell_pepper', 'carrot', 'cauliflower', 'chillies', 'cucumber', 'garlic', 'grapes', 'mango', 'mushroom', 'onion', 'orange', 'pear', 'potato', 'promegranate', 'tomato', 'watermelon']\n",
            "\n",
            "roboflow:\n",
            "  workspace: news\n",
            "  project: vegetables-mr0jj\n",
            "  version: 4\n",
            "  license: CC BY 4.0\n",
            "  url: https://universe.roboflow.com/news/vegetables-mr0jj/dataset/4"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! yolo task=detect mode=train model=yolov8n.pt imgsz=640 data=/content/vegetables/data.yaml epochs=20 batch=8 name=yolov8n_custom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uder2BVuK4c",
        "outputId": "f648606f-49f7-480c-d97b-fd047eff4727"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.217 üöÄ Python-3.10.12 torch-2.1.0+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/vegetables/data.yaml, epochs=20, patience=50, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=yolov8n_custom2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/yolov8n_custom2\n",
            "2023-11-25 16:30:57.178517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-25 16:30:57.178586: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-25 16:30:57.178618: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Overriding model.yaml nc=80 with nc=18\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    754822  ultralytics.nn.modules.head.Detect           [18, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3014358 parameters, 3014342 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/yolov8n_custom2', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/vegetables/train/labels.cache... 1775 images, 0 backgrounds, 0 corrupt: 100% 1775/1775 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/vegetables/train/images/bell_pepper-2823-29_-7C-7C_Y8WkzpirbeHKnUfJKeuBw_jpg.rf.cf9e50ebe4302cc4b07f19a58fd5e9b8.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/vegetables/train/images/bell_pepper-2823-29_-7C-7C_Y8WkzpirbeHKnUfJKeuBw_jpg.rf.fdf346f0e07d898440c33a1407c2f98c.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/vegetables/valid/labels.cache... 256 images, 0 backgrounds, 0 corrupt: 100% 256/256 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/yolov8n_custom2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000455, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/yolov8n_custom2\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/20         0G      1.377      3.739      1.614         44        640: 100% 222/222 [06:48<00:00,  1.84s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:23<00:00,  1.49s/it]\n",
            "                   all        256        636      0.533       0.19       0.17     0.0688\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/20         0G      1.329      2.995       1.54         32        640: 100% 222/222 [06:45<00:00,  1.83s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:22<00:00,  1.38s/it]\n",
            "                   all        256        636      0.494      0.327      0.298      0.117\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/20         0G      1.346      2.648       1.55         32        640: 100% 222/222 [06:43<00:00,  1.82s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:24<00:00,  1.51s/it]\n",
            "                   all        256        636      0.464      0.438      0.386      0.152\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/20         0G      1.318      2.405      1.505         30        640: 100% 222/222 [06:44<00:00,  1.82s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:22<00:00,  1.43s/it]\n",
            "                   all        256        636      0.556      0.449      0.455      0.215\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/20         0G      1.292      2.211      1.489         26        640: 100% 222/222 [06:44<00:00,  1.82s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:24<00:00,  1.52s/it]\n",
            "                   all        256        636      0.581       0.48      0.505      0.216\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/20         0G      1.281      2.094      1.475         31        640: 100% 222/222 [06:42<00:00,  1.81s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:22<00:00,  1.39s/it]\n",
            "                   all        256        636       0.63      0.525      0.539      0.243\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/20         0G      1.251      1.992      1.448         25        640: 100% 222/222 [06:39<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:23<00:00,  1.49s/it]\n",
            "                   all        256        636       0.67       0.53      0.585      0.266\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/20         0G      1.226      1.847      1.428         38        640: 100% 222/222 [06:41<00:00,  1.81s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:23<00:00,  1.50s/it]\n",
            "                   all        256        636      0.632      0.588      0.586      0.244\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/20         0G      1.226      1.802       1.43         37        640: 100% 222/222 [06:39<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:23<00:00,  1.45s/it]\n",
            "                   all        256        636      0.652      0.584      0.594      0.268\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/20         0G      1.176      1.674      1.399         26        640: 100% 222/222 [06:40<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:23<00:00,  1.46s/it]\n",
            "                   all        256        636      0.677      0.588      0.642      0.285\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/20         0G      1.132      1.683      1.428         17        640: 100% 222/222 [06:36<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:23<00:00,  1.44s/it]\n",
            "                   all        256        636      0.638      0.622      0.599      0.279\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/20         0G      1.108      1.554      1.398         17        640: 100% 222/222 [06:36<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:22<00:00,  1.42s/it]\n",
            "                   all        256        636      0.712      0.622      0.646      0.316\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/20         0G      1.076      1.445      1.383         14        640: 100% 222/222 [06:34<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:22<00:00,  1.40s/it]\n",
            "                   all        256        636      0.752      0.622      0.668      0.339\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/20         0G      1.072      1.383      1.374         19        640: 100% 222/222 [06:38<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:23<00:00,  1.46s/it]\n",
            "                   all        256        636      0.653      0.684      0.691      0.335\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/20         0G      1.035      1.285      1.336         26        640: 100% 222/222 [06:38<00:00,  1.80s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:21<00:00,  1.35s/it]\n",
            "                   all        256        636      0.709      0.638      0.674      0.342\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/20         0G      1.023        1.2      1.315         19        640: 100% 222/222 [06:37<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:25<00:00,  1.61s/it]\n",
            "                   all        256        636      0.632      0.674      0.678      0.376\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/20         0G      1.005      1.178      1.308         17        640: 100% 222/222 [06:37<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:23<00:00,  1.46s/it]\n",
            "                   all        256        636      0.695      0.614      0.702      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/20         0G      0.995       1.13      1.308         17        640: 100% 222/222 [06:38<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:23<00:00,  1.46s/it]\n",
            "                   all        256        636      0.673      0.639      0.695      0.373\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/20         0G     0.9684      1.072      1.281         24        640: 100% 222/222 [06:37<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:23<00:00,  1.47s/it]\n",
            "                   all        256        636      0.599       0.75      0.716      0.403\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/20         0G     0.9571      1.026      1.267         13        640: 100% 222/222 [06:35<00:00,  1.78s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:22<00:00,  1.40s/it]\n",
            "                   all        256        636      0.729      0.669      0.725      0.402\n",
            "\n",
            "20 epochs completed in 2.355 hours.\n",
            "Optimizer stripped from runs/detect/yolov8n_custom2/weights/last.pt, 6.3MB\n",
            "Optimizer stripped from runs/detect/yolov8n_custom2/weights/best.pt, 6.3MB\n",
            "\n",
            "Validating runs/detect/yolov8n_custom2/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.217 üöÄ Python-3.10.12 torch-2.1.0+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 168 layers, 3009158 parameters, 0 gradients, 8.1 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 16/16 [00:22<00:00,  1.38s/it]\n",
            "                   all        256        636      0.729      0.669      0.725      0.403\n",
            "                 apple        256         76      0.689      0.882      0.856      0.457\n",
            "                banana        256         25      0.823       0.96      0.965      0.544\n",
            "           bell_pepper        256         74      0.864      0.942      0.962      0.589\n",
            "                carrot        256         27      0.505      0.556      0.625      0.359\n",
            "           cauliflower        256         15          1      0.987      0.995      0.577\n",
            "              chillies        256         16      0.402      0.126      0.325      0.125\n",
            "              cucumber        256         23       0.61       0.87       0.85      0.393\n",
            "                garlic        256          3          1          0      0.677      0.507\n",
            "                grapes        256         49       0.91      0.959      0.971      0.492\n",
            "                 mango        256         17      0.235      0.294      0.212      0.143\n",
            "              mushroom        256         21      0.301      0.429      0.297      0.128\n",
            "                 onion        256         34      0.765      0.912      0.947      0.551\n",
            "                orange        256         30      0.863      0.837      0.854      0.502\n",
            "                  pear        256         81      0.885      0.753      0.876       0.43\n",
            "                potato        256         77      0.735      0.831      0.901      0.475\n",
            "          promegranate        256          1          1          0          0          0\n",
            "                tomato        256         51      0.751      0.828      0.859      0.394\n",
            "            watermelon        256         16      0.788      0.875      0.879      0.581\n",
            "Speed: 2.5ms preprocess, 73.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/yolov8n_custom2\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Carregar um modelo\n",
        "model = YOLO('yolov8n.pt')  # carregar um modelo oficial\n",
        "model = YOLO('/content/runs/detect/yolov8n_custom2/weights/best.pt')  # carregar um modelo personalizado\n",
        "\n",
        "# Validar o modelo\n",
        "metrics = model.val()  # nenhum argumento necess√°rio, conjunto de dados e configura√ß√µes lembrados\n",
        "metrics.box.map    # map50-95\n",
        "metrics.box.map50  # map50\n",
        "metrics.box.map75  # map75\n",
        "metrics.box.maps   # uma lista cont√©m map50-95 de cada categoria\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFOsqDXTfFRV",
        "outputId": "faa48187-58e0-4a8e-95e3-9c3128733945"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.217 üöÄ Python-3.10.12 torch-2.1.0+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 168 layers, 3009158 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/vegetables/valid/labels.cache... 256 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 256/256 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:23<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        256        636      0.729      0.669      0.725      0.403\n",
            "                 apple        256         76      0.689      0.882      0.856      0.457\n",
            "                banana        256         25      0.823       0.96      0.965      0.544\n",
            "           bell_pepper        256         74      0.864      0.942      0.962      0.589\n",
            "                carrot        256         27      0.505      0.556      0.625      0.359\n",
            "           cauliflower        256         15          1      0.987      0.995      0.577\n",
            "              chillies        256         16      0.402      0.126      0.325      0.125\n",
            "              cucumber        256         23       0.61       0.87       0.85      0.393\n",
            "                garlic        256          3          1          0      0.677      0.507\n",
            "                grapes        256         49       0.91      0.959      0.971      0.492\n",
            "                 mango        256         17      0.235      0.294      0.212      0.143\n",
            "              mushroom        256         21      0.301      0.429      0.297      0.128\n",
            "                 onion        256         34      0.765      0.912      0.947      0.551\n",
            "                orange        256         30      0.863      0.837      0.854      0.502\n",
            "                  pear        256         81      0.885      0.753      0.876       0.43\n",
            "                potato        256         77      0.735      0.831      0.901      0.475\n",
            "          promegranate        256          1          1          0          0          0\n",
            "                tomato        256         51      0.751      0.828      0.859      0.394\n",
            "            watermelon        256         16      0.788      0.875      0.879      0.581\n",
            "Speed: 1.6ms preprocess, 79.4ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0.45741,     0.54444,     0.58877,      0.3587,     0.57702,     0.12492,     0.39323,     0.50673,     0.49167,     0.14345,     0.12823,     0.55107,     0.50237,     0.42987,     0.47486,           0,     0.39375,     0.58057])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect predict model='/content/runs/detect/yolov8n_custom2/weights/best.pt' source='/content/tresfrutas03.jpg'"
      ],
      "metadata": {
        "id": "KHbQIAHmqZBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model='/content/runs/detect/yolov8n_custom2/weights/best.pt' format=onnx"
      ],
      "metadata": {
        "id": "xMN6BY9p6rdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "model = YOLO(\"../content/runs/detect/yolov8n_custom2/weights/best.onnx\")\n",
        "\n",
        "im2 = cv2.imread(\"/content/tresfrutas01.jpg\")\n",
        "\n",
        "# Resize the image to the expected dimensions (640x640)\n",
        "im2 = cv2.resize(im2, (640, 640))\n",
        "\n",
        "results = model.predict(source=im2, save=True, save_txt=True, imgsz=640)  # Update imgsz to the resized dimensions\n",
        "\n",
        "for result in results:\n",
        "    boxes = result.boxes  # Boxes object for bbox outputs\n",
        "    masks = result.masks  # Masks object for segmentation masks outputs\n",
        "    probs = result.probs  # Class probabilities for classification outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEEXUSE1_jed",
        "outputId": "a108e4ed-08bf-4ada-8b7b-7816975833b6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ../content/runs/detect/yolov8n_custom2/weights/best.onnx for ONNX Runtime inference...\n",
            "\n",
            "0: 640x640 1 apple, 1 banana, 1 pear, 92.2ms\n",
            "Speed: 2.5ms preprocess, 92.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict12\u001b[0m\n",
            "1 label saved to runs/detect/predict12/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import sys\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")  # path to model file\n",
        "cap = cv2.VideoCapture(0)  # path to video file or webcam\n",
        "crop_filename = \"crop\"  # set the crop file name\n",
        "crop_count = 0\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Error reading video file\")\n",
        "    sys.exit()\n",
        "\n",
        "if not os.path.exists(\"crop\"):\n",
        "    os.mkdir(\"crop\")\n",
        "\n",
        "while cap.isOpened():\n",
        "\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "        results = model.predict(frame, verbose=False)\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        for box in boxes:\n",
        "            crop_count += 1\n",
        "            crop_object = frame[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
        "            cv2.imwrite(os.path.join(\"crop\", crop_filename+ f\"_{crop_count}.png\"), crop_object)\n",
        "\n",
        "        cv2.imshow(\"YOLOv8 Detection\", frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "60l_hpsdwNR4",
        "outputId": "520c63f6-631a-4998-e45b-3bd2b528c4f6"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading video file\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}